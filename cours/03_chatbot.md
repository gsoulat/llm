# Jour 1 - Module 3 : Donnons une M√©moire √† notre IA

Nous savons maintenant envoyer une requ√™te unique √† notre LLM. Pour cr√©er une v√©ritable conversation, il nous faut une pi√®ce ma√Ætresse : la **m√©moire**. Sans elle, notre LLM est amn√©sique et chaque question est un nouveau d√©part. Dans ce module, nous allons construire un chatbot capable de se souvenir du fil de la discussion.

---

## 1. Le D√©fi : l'Amn√©sie des LLMs

Fondamentalement, les LLMs sont "sans √©tat" (*stateless*). Chaque requ√™te que vous envoyez est trait√©e de mani√®re totalement ind√©pendante de la pr√©c√©dente.

* **Vous :** "Quelle est la capitale de la France ?"
* **LLM :** "Paris."
* **Vous :** "Quel est son monument le plus c√©l√®bre ?"
* **LLM :** "De quoi parlez-vous ? 'son' se r√©f√®re √† quoi ?"

Pour que le dialogue soit coh√©rent, nous devons fournir au LLM l'historique des √©changes √† chaque nouvelle question.

![Diagramme montrant une requ√™te simple vs une requ√™te avec historique conversationnel](https://i.imgur.com/x5bJdJd.png)

---

## 2. La Strat√©gie : G√©rer l'Historique de Conversation

La solution consiste √† maintenir une liste des messages √©chang√©s et √† l'envoyer avec chaque nouvelle requ√™te.

### Les Limites de la M√©moire

Cette approche, bien qu'efficace, a une contrainte majeure : la **fen√™tre de contexte** (*context window*). Chaque mod√®le de langage a une limite sur la quantit√© de texte (mesur√©e en *tokens*) qu'il peut analyser en une seule fois.

| Avantages | Inconv√©nients & Limites |
| :--- | :--- |
| ‚úÖ **Coh√©rence :** Le dialogue est fluide et naturel. | ‚ùå **Taille du Contexte :** Si la conversation devient trop longue, les messages les plus anciens sont "oubli√©s". |
| ‚úÖ **Exp√©rience Riche :** L'IA peut se souvenir des pr√©f√©rences de l'utilisateur. | ‚ùå **Co√ªt et Lenteur :** Envoyer un long historique √† chaque fois consomme plus de ressources et peut ralentir la r√©ponse. |

> **Pour aller plus loin :** Pour des conversations tr√®s longues, des techniques avanc√©es existent, comme la **compression d'historique** (un LLM r√©sume la conversation pass√©e) ou l'utilisation d'une m√©moire externe avec **RAG** (que nous verrons bient√¥t !).

---

## 3. Construire notre Chatbot en Python

Le script ci-dessous met en place une boucle interactive qui :
1.  Attend la question de l'utilisateur.
2.  Ajoute la question √† l'historique.
3.  Envoie l'historique complet √† l'API d'Ollama.
4.  Affiche la r√©ponse du LLM.
5.  Ajoute la r√©ponse de l'IA √† l'historique pour pr√©parer le tour suivant.

**Lien vers le fichier de code :** `code/module3_chatbot.py`

```python
import requests
import json

OLLAMA_API_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "llama3"

def chat_with_history(messages):
    """
    Envoie une conversation avec historique √† l'API d'Ollama.
    """
    try:
        data = {
            "model": MODEL_NAME,
            "messages": messages,
            "stream": False
        }
        response = requests.post(OLLAMA_API_URL, json=data)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Erreur de connexion : {e}")
        return None

if __name__ == "__main__":
    # Initialisation de l'historique de la conversation
    conversation_history = []
    print("ü§ñ Votre chatbot est pr√™t ! Tapez 'exit' ou 'quit' pour arr√™ter.")

    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["exit", "quit"]:
            print("Au revoir !")
            break

        # Ajoute le message de l'utilisateur √† l'historique
        conversation_history.append({"role": "user", "content": user_input})

        # Envoie l'historique complet et r√©cup√®re la r√©ponse
        response_json = chat_with_history(conversation_history)

        if response_json:
            # Ajoute la r√©ponse de l'assistant √† l'historique
            assistant_response = response_json['message']
            conversation_history.append(assistant_response)
            print(f"IA : {assistant_response['content']}")
```

### Comment √ßa marche ?

Le secret r√©side dans la structure des `messages`. Nous n'envoyons plus un simple `prompt`, mais une liste d'objets, chacun avec un `role` ("user" ou "assistant") et un `content`. C'est le format standard que les mod√®les de chat comprennent pour suivre une conversation.

---

Vous avez maintenant une application de chat fonctionnelle, la brique fondamentale de tout assistant virtuel.

Ceci conclut notre premi√®re journ√©e ! Nous avons mis en place notre environnement, appris √† interroger le LLM et construit un chatbot. Demain, nous aborderons un concept qui d√©cuple la puissance des LLMs : le **RAG**.

**[‚û°Ô∏è Prochain Module : Introduction au RAG](./04_introduction_rag.md)**