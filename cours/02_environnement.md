# Jour 1 - Module 2 : Mettre en Place Votre Atelier IA Local

Maintenant que nous avons explor√© la th√©orie, il est temps de se salir les mains !  
Dans ce module, nous allons installer l'outil essentiel **Ollama** et √©crire notre tout premier script Python pour dialoguer avec un Grand Mod√®le de Langage (LLM) directement sur notre ordinateur.  
C'est la premi√®re √©tape concr√®te pour donner vie √† vos projets d'IA.

---

## √âtape 1 : Installer Ollama, le C≈ìur de Notre Syst√®me

**Ollama** est un outil formidable qui simplifie radicalement l'ex√©cution de LLMs en local.  
Il s'occupe de toute la complexit√© pour nous.

Pour l'installer, rendez-vous sur la page de t√©l√©chargement officielle et suivez les instructions pour votre syst√®me d'exploitation (Windows, macOS, ou Linux).

‚û°Ô∏è [**Page de T√©l√©chargement d'Ollama**](https://ollama.com/download)

L'installation est simple et rapide.  
Une fois termin√©e, ouvrez un terminal (ou PowerShell sur Windows) et v√©rifiez que tout est en ordre avec la commande :

```bash
ollama --version
```

Vous devriez voir appara√Ætre la version d'Ollama, confirmant que l'installation a r√©ussi.

## √âtape 2 : T√©l√©charger Votre Premier Mod√®le de Langage

Ollama a besoin d'un "cerveau" pour fonctionner.  
Nous allons t√©l√©charger llama3, un excellent mod√®le polyvalent d√©velopp√© par Meta, id√©al pour commencer.

Dans votre terminal, tapez simplement :

```bash
ollama pull llama3
```

Le t√©l√©chargement peut prendre un certain temps, car les mod√®les sont des fichiers volumineux (plusieurs gigaoctets).  
Une fois termin√©, vous pouvez tester le mod√®le directement dans votre terminal pour une conversation instantan√©e :

```bash
ollama run llama3
```

Vous pouvez alors poser une question, par exemple :

```
>>> Pourquoi le ciel est-il bleu ?
```

Pour quitter la conversation, tapez `/bye`.

## √âtape 3 : Dialoguer avec le LLM en Python

Discuter dans un terminal est amusant, mais le v√©ritable pouvoir se r√©v√®le lorsque nous int√©grons le LLM dans nos propres applications.  
Ollama rend cela possible en exposant une API locale sur votre machine, que nous pouvons interroger avec un simple script Python.

### Conseil Pro : Toujours Utiliser un Environnement Virtuel !

Avant d'installer des biblioth√®ques Python, il est crucial d'isoler votre projet.  
Un environnement virtuel (venv) cr√©e une bulle pour votre projet, √©vitant les conflits entre les d√©pendances de diff√©rents projets.

Cr√©ez l'environnement (√† la racine de votre projet) :

```bash
python3 -m venv .venv
```

Activez-le :

**Sur macOS/Linux :**
```bash
source .venv/bin/activate
```

**Sur Windows (CMD) :**
```cmd
.venv\Scripts\activate
```

**Sur Windows (PowerShell) :**
```powershell
.venv\Scripts\Activate.ps1
```

Votre invite de commande devrait maintenant afficher `(.venv)`, indiquant que vous √™tes dans votre environnement isol√©.

### Installation de la Biblioth√®que requests

Pour communiquer avec l'API d'Ollama, nous utiliserons la biblioth√®que requests, un standard en Python pour effectuer des requ√™tes HTTP.

```bash
pip install requests
```

### Notre Premier Script d'Interaction

Voici un script simple pour poser une question au mod√®le llama3 et afficher sa r√©ponse.

üìÑ **Lien vers le fichier de code :** `code/module2_premier_script.py`

```python
import requests
import json

# L'URL de l'API locale d'Ollama
OLLAMA_API_URL = "http://localhost:11434/api/chat"

def query_llm(prompt):
    """
    Envoie une requ√™te au LLM via l'API d'Ollama et retourne la r√©ponse.
    """
    try:
        # Les donn√©es √† envoyer dans la requ√™te POST
        data = {
            "model": "llama3",  # Le mod√®le que nous voulons utiliser
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": False  # Pour recevoir la r√©ponse en une seule fois
        }

        # Envoi de la requ√™te POST
        response = requests.post(OLLAMA_API_URL, json=data)
        response.raise_for_status()  # L√®ve une exception pour les codes d'erreur HTTP

        # Extraction et affichage de la r√©ponse
        response_json = response.json()
        print("R√©ponse du LLM :", response_json['message']['content'])

    except requests.exceptions.RequestException as e:
        print(f"Erreur de connexion √† l'API d'Ollama : {e}")
        print("Veuillez vous assurer qu'Ollama est en cours d'ex√©cution.")

if __name__ == "__main__":
    user_prompt = "Explique le concept de trou noir de mani√®re simple."
    query_llm(user_prompt)
```

**Note importante :**  
Pour que ce script fonctionne, le service Ollama doit √™tre lanc√© sur votre machine.  
Si le script √©choue avec une erreur de connexion, c'est probablement que l'application Ollama n'est pas active.

## üéâ F√©licitations !

Vous avez franchi une √©tape majeure.  
Vous disposez maintenant d'un LLM fonctionnant en local et vous savez comment interagir avec lui en utilisant Python.  
Cette comp√©tence est la fondation sur laquelle nous allons construire des applications de plus en plus sophistiqu√©es.

‚û°Ô∏è **Prochain Module :** Construire un Chatbot simple