# Jour 2 - Module 9 : Interroger sa Base de Connaissances avec RAG

Nous avons une base de connaissances vectorielle pr√™te √† l'emploi. Il est maintenant temps de l'utiliser pour ce pour quoi elle a √©t√© con√ßue : trouver des informations pertinentes et les fournir √† notre LLM afin qu'il puisse r√©pondre √† nos questions de mani√®re factuelle.

C'est ici que la magie du RAG op√®re. Nous allons mettre en ≈ìuvre la deuxi√®me phase majeure de notre syst√®me : l'**interrogation**.

## 1. Le Processus d'Interrogation RAG

Le script que nous allons construire va orchestrer le flux de travail complet du RAG √† chaque fois que l'utilisateur pose une question. Ce processus se d√©roule en trois √©tapes cl√©s, qui sont la mise en application de la th√©orie vue pr√©c√©demment.

![Diagramme simple expliquant le processus RAG en 3 √©tapes : Recherche, Augmentation, G√©n√©ration](images/rag.png)

1.  **√âtape de Recherche (Retrieval)** : Nous utilisons la question de l'utilisateur pour interroger la base ChromaDB.
    * ChromaDB transforme la question en vecteur (en utilisant le m√™me mod√®le d'embedding que celui utilis√© pour les documents).
    * Il compare ce vecteur √† tous les vecteurs de *chunks* stock√©s.
    * Il retourne les *chunks* les plus similaires, c'est-√†-dire le contexte le plus pertinent.

2.  **√âtape d'Augmentation (Augmented)** : Nous construisons dynamiquement un nouveau prompt pour le LLM. Ce prompt est un assemblage de plusieurs √©l√©ments :
    * Une **instruction** claire pour le LLM (un "system prompt").
    * Le **contexte** retrouv√© √† l'√©tape pr√©c√©dente.
    * La **question originale** de l'utilisateur.

3.  **√âtape de G√©n√©ration (Generation)** : Ce prompt "augment√©" est envoy√© au LLM (via l'API d'Ollama), qui g√©n√®re la r√©ponse finale en se basant sur le contexte fourni.

---

## 2. Impl√©mentation Manuelle avec ChromaDB et Ollama

Cette premi√®re approche permet de bien comprendre chaque rouage du m√©canisme RAG. Nous allons tout coder explicitement.

**Lien vers le fichier de code complet :** [`09_rag_complet.py`](./09_rag_complet.py)

### √âtape 1 : Recherche (Retrieval)

Tout commence par la fonction `rag` qui prend la question de l'utilisateur. La premi√®re action est d'interroger la collection ChromaDB pour trouver les documents les plus pertinents.

```python
# Fichier : 09_rag_complet.py

def rag(question):
    """
    Ex√©cute le processus RAG complet : recherche, augmentation, g√©n√©ration.
    """
    # 1. √âTAPE DE RECHERCHE (Retrieval)
    # Interroge la collection ChromaDB pour trouver les 3 chunks les plus pertinents.
    # n_results=3 : on demande les 3 r√©sultats les plus proches.
    print("   -> 1. Recherche des documents pertinents...")
    results = collection.query(query_texts=[question], n_results=3)

    # R√©cup√®re le contenu des documents (chunks) trouv√©s.
    context = "\n".join(results["documents"][0])
```

### √âtape 2 : Augmentation (Augmented)

Avec le contexte en main, nous utilisons un template pour formater un prompt clair et pr√©cis. C'est une √©tape cruciale pour guider le LLM et r√©duire les risques d'hallucination.

üí° **Bonne pratique : Le Prompt Template**
Isoler le prompt dans un template rend le code plus lisible et facilite grandement les exp√©rimentations. Changer le prompt est l'une des mani√®res les plus efficaces d'am√©liorer les performances de votre RAG.

```python
# Fichier : 09_rag_complet.py (suite)

    # 2. √âTAPE D'AUGMENTATION (Augmented)
    # Cr√©e le template du prompt. L'instruction "En te basant uniquement sur le contexte suivant"
    # est essentielle pour forcer le mod√®le √† utiliser les documents fournis.
    print("   -> 2. Augmentation du prompt...")
    prompt_template = (
        "En te basant uniquement sur le contexte suivant, r√©ponds √† la question.\n\n"
        "--- Contexte ---\n"
        "{context}"
        "--- Fin du Contexte ---\n\n"
        "Question: {question}"
    )
    # Remplit le template avec le contexte et la question.
    prompt = prompt_template.format(context=context, question=question)
```

### √âtape 3 : G√©n√©ration (Generation)

Le prompt final est envoy√© √† l'API de chat d'Ollama. Nous r√©cup√©rons la r√©ponse et la retournons √† l'utilisateur.

```python
# Fichier : 09_rag_complet.py (suite)

    # 3. √âTAPE DE G√âN√âRATION (Generation)
    # Pr√©pare les donn√©es pour l'API de chat d'Ollama.
    print("   -> 3. G√©n√©ration de la r√©ponse par le LLM...")
    data = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
    }

    # Envoie la requ√™te au LLM.
    response = requests.post("http://localhost:11434/api/chat", json=data)

    # Retourne la r√©ponse du LLM.
    if response.status_code == 200:
        return json.loads(response.text)["message"]["content"]
    else:
        return f"Erreur lors de la g√©n√©ration : {response.text}"
```

Lancez ce script (`09_rag_complet.py`) et posez des questions sur le document que vous avez ing√©r√© (par exemple sur les "conventional commits" si vous avez utilis√© le PDF fourni). Vous verrez le processus se d√©rouler en direct.

## 3. Impl√©mentation avec LangChain : Une Approche Modulaire

LangChain est un framework puissant qui simplifie le d√©veloppement d'applications bas√©es sur les LLMs. Il offre des abstractions pour cha√Æner les composants de mani√®re plus propre et r√©utilisable.

### Pourquoi utiliser LangChain ?

*   **Modularit√©** : Chaque √©tape (retriever, prompt, mod√®le) est un objet distinct, facile √† remplacer.
*   **Int√©grations** : Connexion simplifi√©e √† des centaines de bases de donn√©es, LLMs et outils.
*   **Lisibilit√©** : Le LangChain Expression Language (LCEL) permet de d√©finir des cha√Ænes complexes avec une syntaxe `|` (pipe) tr√®s intuitive.

**Lien vers le fichier de code :** `code/module6_rag_langchain.py` (nomm√© dans le document de r√©f√©rence).

### Comment √ßa fonctionne avec LangChain ?

Le code est plus concis car LangChain g√®re une grande partie de la complexit√©.

1.  **Initialisation des composants** : On cr√©e des objets pour le LLM, les embeddings et le retriever (notre base ChromaDB).
2.  **D√©finition du Prompt** : On utilise `ChatPromptTemplate` pour une gestion structur√©e du prompt.
3.  **Cr√©ation de la Cha√Æne (Chain)** : On utilise l'op√©rateur `|` pour assembler la cha√Æne. La lisibilit√© est imm√©diate : le contexte du retriever est "pip√©" dans le prompt, qui est ensuite "pip√©" dans le mod√®le, dont la sortie est finalement pars√©e.

```python
# Extrait conceptuel du code LangChain

# 1. Transformer la base de donn√©es en un "Retriever"
retriever = db.as_retriever()

# 2. D√©finir le template de prompt
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# 3. D√©finir le mod√®le
model = ChatOllama(model="llama3.2:latest")

# 4. Cr√©er la cha√Æne RAG avec LCEL (le | est un "pipe")
chain = (
    RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    )
    | prompt
    | model
    | StrOutputParser()
)

# 5. Invoquer la cha√Æne
response = chain.invoke("What are conventional commits?")
```

Cette approche est non seulement plus √©l√©gante mais aussi beaucoup plus facile √† maintenir et √† faire √©voluer.

F√©licitations ! Vous avez construit et utilis√© un syst√®me RAG complet de deux mani√®res diff√©rentes. C'est une comp√©tence extr√™mement pr√©cieuse qui vous permet de cr√©er des applications d'IA capables de raisonner sur des donn√©es priv√©es et v√©rifiables.